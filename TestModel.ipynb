{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaaa\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['fft']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from util_functs import *\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os.path import isdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import h5py\n",
    "\n",
    "from keras.models import load_model\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we test the trained model on the test data and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model on the unseen test data. \n",
    "\n",
    "#Define the path to the model and the path to the 6-second audio clips\n",
    "model_path = 'models/model_94t.h5'\n",
    "audio_path = 'input_clips/'\n",
    "\n",
    "#Generate the image & label data - as well as the dataframe containing filename-label pairs - using the\n",
    "#generate_data function\n",
    "X_test, y_test, ms_df_test = generate_data((34,50,4), ms_dir='test_ms/', csv_name='test.csv')\n",
    "\n",
    "#Load the model & make predictions on the test data. The softmax predictions need to be coverted to index \n",
    "#predictions to calculate true accuracy\n",
    "model = load_model(model_path)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis=1) + 1\n",
    "predictions = predictions.reshape(len(ms_df_test),1)\n",
    "\n",
    "#Calculate the difference between the predictions and true labels, then find where the difference is 0 (correct predictions) \n",
    "diff_array = predictions - y_test\n",
    "correct_preds = diff_array[diff_array == 0]\n",
    "\n",
    "#Finally, calculate the accuracy of the model on the test data\n",
    "acc_percent = (len(correct_preds) / len(diff_array)) * 100\n",
    "acc_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does well, and achieves ~94.5% accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick demonstration of the predictions on the test data\n",
    "i=random.randint(len(ms_df_test))\n",
    "prediction = np.argmax(model.predict(np.array([X_test[i]]))[0]) + 1\n",
    "\n",
    "print(\"For the image {}, the model predicts that the song is {}\".format(ms_df_test.iloc[i].fname, str(prediction) + \": \" + LABEL_DICT[prediction]))\n",
    "\n",
    "#PLay clip associated with above image\n",
    "filename = audio_path + (os.path.splitext(ms_df_test.iloc[i].fname)[0] + '.wav')\n",
    "clip, sample_rate = librosa.load(filename, sr=None)\n",
    "Audio(clip, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion/Future Work\n",
    "\n",
    "The model trained here used only one voice as its input source. Obviously, better generalization could be achieved if more voices are used. The songs chosen here were carefully chosen: they contain unique, distintive frequency patterns. The assumption was that not many voices need be used when trainng on these songs since the frequency patterns of the 6 second recordings should look similar accross gender & race. Moreover, the approach being used clearly cannot be used at scale (millions of songs + more being released every day means re-training model all the time!!), but it serves as a cool little approach to use for a pet project such as this :)\n",
    "\n",
    "With that being said, clear avenues to explore next would be to incorporate more voices into training, and particularly, incorporate female voice. Originally, 5 voices were recorded for the project, but the audio quality was not great for the other recordings. Thus, for the purposes of this project, only the one voice was used, but it would be very easy to get other people to record high-quality audio and improve the generalization of the model despite the contraints of little data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a little aside, you can try the model with your own clips! Simply specify the location of the clip & directories to store your raw recodings, input clips and input melspecs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT LOCATION OF CLIP HERE ##\n",
    "your_clip_name = '3.mp3'\n",
    "##################################\n",
    "## SPECIFY RAW AUDIO DIR (your_recording_dir), INPUT CLIP DIR (your_clip_dir), INPUT MELSPEC DIR (your_ms_dir) ##\n",
    "your_recording_dir = \"your_recordings/\" \n",
    "your_clip_dir = \"your_audio_clips/\"\n",
    "your_ms_dir = \"your_ms/\"\n",
    "\n",
    "#-------------------------------------------------------#\n",
    "\n",
    "# Load in the clip and write it to .wav format\n",
    "clip, sample_rate = librosa.load(your_recording_dir + your_clip_name, sr=None, duration=6)\n",
    "librosa.output.write_wav(your_clip_dir + os.path.splitext(your_clip_name)[0] + \".wav\", clip, sample_rate)\n",
    "\n",
    "#Generate the mel-spectrograms from the clips\n",
    "generate_melspecs(input_dir=your_clip_dir, ms_output=your_ms_dir)\n",
    "\n",
    "#Generate the input data\n",
    "X, _ = initialise_data(img_folder=your_ms_dir, image_shape=(34,50,4))\n",
    "img_name = os.path.splitext(your_clip_name)[0] + '.png'\n",
    "img = imageio.imread(your_ms_dir + img_name)\n",
    "X[0] = img\n",
    "\n",
    "#Calculate the prediction on the input image, then get the best and second best predictions from the softmax vector\n",
    "prediction = model.predict(np.array([X[0]]))[0] #softmax vector \n",
    "ind = np.argpartition(prediction, -2)[-2:] #get indices of top two preds\n",
    "best_guess = ind[np.argsort(prediction[ind])][1] + 1\n",
    "second_best = ind[np.argsort(prediction[ind])][0] + 1\n",
    "\n",
    "#Print the predictions\n",
    "print(\"For the image {}, best guess: {}, 2nd best guess: {}\".format(img_name, LABEL_DICT[best_guess], LABEL_DICT[second_best]))\n",
    "\n",
    "#PLay clip associated with above image\n",
    "filename = your_clip_dir + os.path.splitext(your_clip_name)[0] + \".wav\"\n",
    "clip, sample_rate = librosa.load(filename, sr=None)\n",
    "Audio(clip, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Fast.ai experimental audio classification module: https://github.com/sevenfx/fastai_audio <br/>\n",
    "[2] Article for fast.ai audio module: https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89 <br/>\n",
    "[3] A CNN architecture for classifying digits with spectrograms: https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e <br/> https://github.com/Jakobovski/free-spoken-digit-dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
